%*************************************************************
% Master Thesis                                             *
% Ing. Minerva Gabriela Vargas Gleason                       *
%  IAI - Institute of Artificial Intelligence                 *
% Universit√§t Bremen                                         *
%                                                            *
% pdfLaTex                                                   *
% Editor: TeXnicCenter                                       *
%*************************************************************


\chapter{\textbf{Background \&  Related Work}}

This chapter presents the current state of the art of some relevant topics for mobile robotics that the reader needs to fully understand the content of this thesis. The first section will refer to the kinematic model of a robot. Afterwards, the general concepts of trajectory planning will be explained. In the end, a brief explanation of the software used in this thesis is given.

%\section{Localization Algorithms (\textit{if required})}
%\todo{do I need this section?}
%Robot localization can be defined as the on-line estimation of a mobile robot's position based on sensor data; the position is given with respect to a global coordinate frame. Nowadays, localization is a fundamental research problem in robotics since it is required for most mobile robotics' tasks.
%
%According to \citet{Markov}, the aim of \textit{localization} is to estimate the position of a robot, given a map of the environment and data from the robot's sensors. Localization is often referred to as \textit{position estimation}.
%
%The current localization techniques can be separated on two groups according to the problem they solve (\citep{Markov}):
%\begin{itemize}
%	\item \textbf{Tracking or local:} In this case, the initial position of the robot is known and the algorithm only has to compensate for odometry errors that appear when the robot moves. The main problem they present is that they cannot recover if they lose track of the robot's position (considering some threshold).
%	\item \textbf{Global:} These are able to obtain the initial position of a robot. In other words, they estimate the robot's position under global uncertainty. This is commonly known as the \textit{wake-up robot problem}, where the initial location of the robot is unknown.
%\end{itemize}
%
%A well-known problem in robot localization is known as the \textit{kidnapped robot problem}, where a robot is carried to a different location. Hence the robot must be able to determine whether the previous known position is wrong and,if it is, find its current location. Global algorithms are able to solve this problem. 
%
%\subsection{Markov Localization}
%
%The \textit{Markov Localization} is a global technique that estimates the position of the robot based on a probabilistic framework. Basically, what Markov localization does is represent the robot's belief by a probability distribution over possible positions \citep{Montecarlo}. The belief is updated using Bayes rule every time the robot moves or receives data from its sensors. 
%
%Let us denote the robot's position by  $l=(x,y,\theta)$, where $x$ and $y$ are the robot's coordinates with respect to a fixed world reference frame, and $\theta$ is the robot's orientation. The probability distribution that states the robot's belief of being at a certain position $l$ is given by $Bel(l)$.
%
%As shown in Figure \ref{fig:markov}, the robot starts with a uniform distribution of belief states representing that it is equally probable for the robot to be in any position in the environment. After receiving data from the sensors, the belief $Bel(l)$ is updated and the more probable positions obtain a higher value. Eventually, the robot becomes highly certain about its position, which is represented by a narrow Gaussian distribution centered around the robot's current location.
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.4\linewidth]{markov.png}
%	\vspace{-10pt}
%	\caption[Markov localization: basic concept.]{Markov localization: basic concept. \citep[chap. 2, page 393]{Markov}}
%	\vspace{-15pt}
%	\label{fig:markov}
%\end{figure}
%
%As explained by \citet{Montecarlo}, the belief is updated with two different probabilistic models: a model that represents the robot's motion and a perception model that considers the sensor readings:
%\begin{itemize}
%	\item \textbf{Robot motion:} The probability that a measured movement $a$ occurs at a certain position $l'$ is given by $P\ (l\ |\ l',\ a)$. This model is used to update the belief state every time a movement is executed.
%	\begin{equation}
%	Bel(l) \leftarrow \int P\ (l\ |\ l',\ a)\ Bel(\ l')\ dl' 
%	\end{equation}
%	\item \textbf{Sensor readings:} The probability of receiving a sensor reading $s$ when the robot is at the location $l$ is given by $P\ (s\ |\ l)$. We can use this to update the robot's belief with:
%	\begin{equation}
%	Bel(l) \leftarrow \alpha \ P\  (s \ |\ l \ )\ Bel(l)
%	\end{equation}
%	where $\alpha$ is a normalization factor that makes $Bel(l)$ integrate to 1.
%\end{itemize}
%
%Markov localization is able to globally estimate the position of the robot, recover from uncertainties and odometry errors and solve the kidnapped robot problem (re-localize the robot). 
%
%This localization technique uses a probability distribution over the space of all hypotheses of where the robot can be ($Bel(l)$) to estimate the current robot location. The computational resources needed to maintain this probability distribution for all possible positions increase with the size of the space. For normal environments, the required memory can easily exceed 100MB. The problem with the high computational cost also appears in \textit{grid-based} Markov localization approaches, which discretize the relevant parts of the space using an evenly spaced grid of points.
%
%This technique works under the assumption that the environment is static (\textit{Markov assumption}), which is not true in some applications of mobile robotics. Sometimes, the robot can be located in an environment where it interacts with people and objects that can be moved. To solve the localization problem in this case, a different technique is required.
%
%\subsection{Monte Carlo Localization}
%
%Monte Carlo Localization (MCL) is a version of sampling-importance-resampling (SIR) based on Markov localization. It is a sample-based algorithm that uses particle filters to ensure the survival of the fittest.
%
%According to \citet{Montecarlo}, this algorithm uses fast sampling techniques to represent the robot's belief $Bel(l)$ by estimating the posterior distribution every time the robot moves or senses using the importance re-sampling technique.
%
%  MCL uses many samples during the global localization and the sample set size is significantly reduced during tracking when an approximate position of the robot is already known. Compared to grid-based Markov localization, MCL requires considerably less memory and, therefore, can integrate measurements at a higher frequency. It is more accurate and easier to implement than Markov localization. The main advantage MCL has is that it is able to perform \textit{global localization}, meaning that MCL can obtain the initial position of a robot and solve the \textit{kidnapped robot problem} at a low computational cost. This allows us to use MCL in bigger environments.
%
%The key concept of this approach is to represent the posterior belief $Bel(l)$ by a set of $N$ weighted, random particles $S=\lbrace s_i\  \vert \ i=1...\ N\rbrace$. This sample set is a discrete approximation of a probability distribution. The distribution is updated by updating the weight of each particle after every sensor's reading or robot's movement.
%
%The samples are represented by $\langle\langle x,\ y,\ \theta\rangle,\ p\rangle$, where the first three parameters denote a possible robot position  and $p$ is the weighting factor of the particle ($p\geq\ 0$). Assume $\sum_{n=1}^{N} p_n\ = 1$, all weights must be normalized so that they sum up to 1.
%
%As explained by \citet{Montecarlo}, updating the particle set is done in two different ways:
%\begin{itemize}
%	\item \textbf{Robot Motion:} Every time the robot moves, $N$ new samples are randomly created. These new samples approximate the robot's position with respect to the motion command. Each new sample is generated from the previous sample set with it's likelihood determined by the particle's weight. The location of the new sample is given by:
%	\begin{equation}
%	\vspace{-20pt}
%		P\ (l\ \vert l'\ ,\ a)
%	\end{equation}
%	where $a$ is the observed movement and $l'$ the previous location. Figure \ref{fig:motion} shows the behavior of this sampling technique where the robot starts with a known position and moves following the straight lines.
%	
%	The uncertainty of the sample sets increases with each iteration. This uncertainty represents the error in the robot's location due to slippage and drift.
%	\item \textbf{Sensor readings:} The sample set is re-weighted implementing Bayes' rule. For a sample $\langle\ l,\ p\ \rangle$, the update rule is:
%	\begin{equation}
%	\vspace{-20pt}
%	p \leftarrow \alpha \ P\  (s \ |\ l \ )
%	\end{equation}
%	where $s$ is the sensor information and $\alpha$ a normalization factor that ensures the weights satisfy $\sum_{n=1}^{N} p_n\ = 1$.
%\end{itemize}
%
%\begin{figure}[H]
%		\centering
%		\includegraphics[width=0.5\linewidth]{robot_location.png}
%		\vspace{-10pt}
%		\caption[Monte Carlo Loc. approximation]{MCL approximation of the position's belief based only on motion commands \citep[page 3]{Montecarlo}.}
%		\vspace{-15pt}
%		\label{fig:motion}
%\end{figure}
%
%In every iteration, the sample set is centered around the robot's believed position, in case of an error in location (such as in the \textit{kidnapped robot problem}, the robot wouldn't be able to re-localize itself. In order to avoid this problem, after each update some random, uniformly distributed samples are added to the set. These added samples are used in case the robot loses track of its position. The random samples uniformly distributed in the whole environment can effectively re-localize the robot. In case the position of the robot is not lost, the weight of these random samples will decrease and the samples will disappear in the next iteration.
%
%\begin{figure}[H]
%	\centering
%	\begin{subfigure}[][Initialization]
%		{\includegraphics[width=0.32\linewidth]{position1.png}}
%	\end{subfigure}
%	\begin{subfigure}[][Uncertainty due to symmetry]
%		{\label{subfig:goal}
%		\includegraphics[width=0.32\linewidth]{position2.png}}
%	\end{subfigure}
%	\begin{subfigure}[][Final localization]
%		{\label{subfig:goal}
%		\includegraphics[width=0.32\linewidth]{position3.png}}
%	\end{subfigure}
%	\vspace{-12pt}
%	\caption{Global localization of a robot using MCL}
%	\vspace{-15pt}
%	\label{fig:example}
%\end{figure}
%\vspace{-8pt}
%
%Figure \ref{fig:example} shows an example of a global localization performed by a robot using MCL. The first image shows the initial sample set distribution where the robot's position is unknown and the samples are uniformly distributed throughout the environment. The second image shows two probable robot positions after the robot has slightly moved; this uncertainty is caused by a symmetry in the map. The random samples in the second image are the ones added after each update. The third image shows the successful localization of the robot after it kept going forward. The information used to achieve this localization was the movement commands and the sensor information.
%
%Nowadays, there are several programs that provide localization capabilities using simultaneous localization and mapping (SLAM). SLAM is the process of constructing a map of an unknown environment while keeping track of the agent's location in it. The agent is normally a robot or an autonomous vehicle.
%
%In the next section, a new SLAM software will be presented: \textit{Google Cartographer}. Google cartographer can obtain information from two different types of sensors, namely sonar and laser scanners, and can be implemented in virtually any mobile robot.
%
%\subsection{Google Cartographer}
%\todo{Do i need this?}
%----- \textbf{TODO: Enter some description here and explain it was used to create a map of the lab used to localize the robot. Gives better results than the previously used software} -----

\section{Kinematic Modelling of a Robot}

According to \citet{Handbook}, \textit{robot kinematics} refers to the motion of the elements in a robot without considering the forces and torques that generated this movement. 
To describe the movement of a robot, we need a kinematic model of its structure. The position and orientation of a body  in space is known as \textit{pose}.

A robot can be modeled as a kinematic chain, which consists of a system of rigid bodies connected by joints, a \textit{kinematic joint} is a connection between 2 bodies that constrains their relative motion. In robotics, these bodies are usually referred to as \textit{links}.  The kinematic description of a robot normally uses some simplifications: the links that form the robot are assumed to be rigid, and each link is ideally connected with the next one, with no gap in between.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth, angle=0]{6dof-robot.png}
	\vspace{-10pt}
	\caption[Example of a 6 DOF manipulator]{Example of a 6 DOF manipulator. \citep[chap. 1, page 24]{Handbook}}
	\vspace{-15pt}
	\label{fig:kinematic}
\end{figure}

Figure \ref{fig:kinematic} shows an example of a serial chain manipulator, where a reference frame is attached to each link. Following the Denavit-Hartenberg convention, the \textit{Z} axis is always aligned with the joint axis \citep{Craig}.

The study of robot kinematics involve how the location of the reference frames change as the mechanism moves. The main goal is to compute the position and orientation of the manipulator's end-effector with respect to the base as a function of the joints values.

For an open-loop robotic mechanism, the general structure is represented by a kinematic tree, which consists of the concatenation of links and joints with a reference frame in each joint \citep{Handbook}. This representation is used to obtain a mathematical model of the system. 

When programming a robot, a suitable representation of its kinematic model is required. The Unified Robot Description Format (URDF) is a standard XML representation of the robot model in the ROS community\footnote{http://wiki.ros.org/urdf}, which includes the robot kinematics, dynamics, and sensors. % In this project, I represented the model of Boxy with a URDF. A section of the kinematic tree of Boxy is found in Appendix \ref{A1}, it shows the neck (UR3) description.

\section{Collision Environment}

\textbf{Workspace}

The workspace of a robot is defined as the total volume swept out by the end-effector as the manipulator executes all possible motions \citep[chap. 1]{Handbook}. In simple terms, the workspace is the space that can be reached by the robot.

\textbf{Working environment}

The working environment consists of all the external factors surrounding and affecting the robot. In other words, it is the space where the robot is working, including all the objects in there. According to \citet{Taskspace}, the robot is considered to be operating in a "world" that it cannot leave; actions can affect the environment and, thus, change it for the robot. 

The environment can be represented in a graphical way with meshes that describe the geometry of the objects and of the robot. The position of the meshes in the model environment has to match the position of the real objects. Motion planning algorithms are then used to calculate trajectories that the robot can execute without colliding with the environment or itself. 

\textbf{Sensors}

A robot can have two types of sensors \citep[chap. 1]{Russel}:
\begin{itemize}
	\item \textbf{Proprioceptive sensors:} Get information about the internal state of the robot, such as the angular position of each joint or the temperature of the links. These sensors are used for self maintenance and control of internal status. Examples of proprioceptive sensors are: shaft encoders, inertial navigation systems and force sensors.
	\item \textbf{Exteroceptive sensors:} Obtain information from the robot's environment, such as the distance to an object relative to a frame of reference of the robot. Many exteroceptive sensors are sensors that can be used to calculate distances. These sensors can be further categorized in contact, range, and vision sensors.
\end{itemize}

The robot takes the information from proprioceptive sensors to calculate the position and orientation of each link and determine its current configuration, and the information from exteroceptive sensors to calculate its position relative to the environment and the distance to surrounding objects. Combining both, the motion planning algorithms can iterate until a collision-free path between the initial and desired position is found.

\section{Trajectory Planning}

According to \citet[chap. 1, page 6]{trajectory} \textit{"A trajectory is the path that a moving object follows through space as a function of time"}. A trajectory can be described as a time-stamped series of location points.

To define a robot's trajectory, we must define the trajectory that each link will execute to bring the robot to a desired pose. Once the robot's kinematic model and environment are defined, we use a motion planning algorithm to solve the path planning problem and obtain a collision-free trajectory for the robot. Planning involves determining the path and the velocity function for a robot. 

\subsection{Euclidean Space in Cartesian Coordinates}

In geometry, we can describe the position of any object in space using Cartesian coordinates (X, Y, and Z). For a 3-dimensional space (Euclidean space), any object can move and rotate along 3 axes, this means that the object has 6 degrees od freedom (DOF), so 6 values are required to define the pose of an object with respect to a reference point. In other words, for a three-dimensional Euclidean space, we can describe the position and orientation of any object using the Cartesian coordinates and three rotation angles.

\subsection{Configuration Space (C-space)}
\label{subsec:cspace}

A complete description of the robot's geometry and of the workspace is needed to solve the path planning problem. A \textit{configuration} $\bm{q}$ specifies the location of every point of the robot's geometry. 

The \textit{configuration space}, where \( \bm{q} \in  \bm{C} \), is the space of all possible configurations \citep{Handbook}. It represents all possible transformations that can be applied to the robot given its kinematics. The C-space gives an abstract way of solving the planning, the main advantage of this representation is that a robot can be mapped into the C-space as a single point, where the number of DOF of the robot is the dimension of the C-space. This is also the minimum number of parameters required to describe a configuration, so motion planning for the robot is equivalent to motion planning for the C-space.

During trajectory planning, one must consider several constraints involving the robot's pose. Since the allowed configurations of the robot are not known beforehand, the planning algorithm must find these valid configurations while planning. Finding these configurations is normally done through sampling, this process can become inefficient for complex environments.

%\subsection{Task Space}
%
%According to \citet{Taskspace}, the \textit{task space representation} (TSR) are general representations of pose constraints that can be efficiently sampled for collision evaluation. TSRs can be chained to describe constraints of articulated objects, like a robot. These representation allows handling complex pose constraints using sampling algorithms for trajectory planning. 

\section{Motion Planning Algorithms}
\todo{write more, add more references, aprox 30 at the end}

One of the fundamental problems in robotics is to plan motions for complex bodies from an initial pose to a goal. As explained by \citet{Handbook}, the general path planning problem is computing a continuous free path for the robot between $\bm{q_{1}}$ and $\bm{q_{G}}$, given:
\begin{enumerate}
	\vspace{-5pt}
	\item The robot's workspace $\bm{W}$
	\vspace{-5pt}
	\item An obstacle region $\bm{O} \subset \bm{W}$
	\vspace{-5pt}
	\item A robot defined as a collection of \textit{m} links: $A_{1}, A_{2}, ... , A_{m}$
	\vspace{-5pt}
	\item The C-space $\bm{C}$ with defined $\bm{C_{obs}}$ and $\bm{C_{free}}$
	\vspace{-5pt} 
	\item An initial configuration $\bm{q_{1}} \in  \bm{C_{free}}$
	\vspace{-5pt} 
	\item A goal configuration $\bm{q_{G}} \in  \bm{C_{free}}$
\end{enumerate}

Where $\bm{C_{obs}}$ is the \textit{C-space obstacle region} and $\bm{C_{free}}$ is the set of configurations that avoid collision, called \textit{free space}. We must compute a continuous free path for the robot between $\bm{q_{1}}$ and $\bm{q_{G}}$.

The main complication is calculating $\bm{C_{obs}}$ and $\bm{C_{free}}$, that are needed to determine the region where the robot can move without colliding. There are two main approaches to solve the planning problem: sampling-based and combinatorial motion planning.

In this project, a controller developed at the Institute for Artificial Intelligence\footnote{http://ai.uni-bremen.de/} (IAI), \textit{Giskard} (section \ref{subsec:giskard}), will be used as robot motion controller.

Motion planners have several ways of approaching the problem and finding a suitable trajectory for the robot to execute. Some algorithms are: sampling-based motion planning and optimization based motion planning.

\subsection{Sampling-based motion planning}
\label{subsec:planning}

The planner samples different configurations in the C-space to construct collision-free paths. These paths are stored as 1D C-space curves. The main idea is to avoid the direct construction of the obstacle region $\bm{C_{obs}}$. This means that instead of considering the obstacles directly, the planner uses a collision detector for each pose in the trajectory. This approach allows using the planner for a wide range of applications, one must only adapt the collision detector to the geometry of a specific robot.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{sampling_alg.png}
	\vspace{-10pt}
	\caption[Sampling-based planning]{Sampling-based planning philosophy \citep[chap. 5, page 185]{planning}.}
	\vspace{-15pt}
	\label{fig:sampling}
\end{figure}

The sampling-based planning, shown in figure \ref{fig:sampling}, uses the collision detection as a "black box" between the motion planning and the geometric model of the robot. This generates an algorithm that is independent of the robot's geometry \citep{planning} .

This "black box" approach can solve problems that involve thousands of geometric primitives representing the robot. It is practically impossible, according to \citet{Handbook}, to solve such problems with algorithms that uses the $\bm{C_{obs}}$ directly.

The disadvantage of these algorithms is that they don't assure to find a solution in a finite amount of time. Combinatorial motion planning algorithms are able to return a solution, if it exists, in a finite amount of time.

A \textbf{rapidly exploring random tree (RRT)} is a sampling-based planning algorithm that searches a collision-free path by randomly building a space-filling tree. It has a good performance and does not require any parameter tuning. As shown in figure \ref{fig:rrt}, RRT reaches unexplored regions after just a few iterations. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{rrt.png}
	\vspace{-10pt}
	\caption[Rapidly exploring random tree]{Rapidly exploring random tree \citep[chap. 5, page 230]{planning}.}
	\vspace{-15pt}
	\label{fig:rrt}
\end{figure}
The basic idea of RRT is to incrementally build a search tree in the C-space that tries to connect $\bm{q_{1}}$ and $\bm{q_{G}}$, avoiding the obstacles in the way, where $\bm{G}$ is the search graph. All the vertices of $\bm{G}$ are collision-free configurations. 

% instead of commented section:
%  RTT will try fo find a path in the tree that connects the starting point with the goal, thus generating a collision-free trajectory for the manipulator.

The steps of this RRT algorithm for a C-space with no obstacles are shown in Algorithm \ref{algor:rrt_alg} \citep{planning}. Let $\bm{S}\subset \bm{C_{free}}$ be the set of all points reached by $\bm{G}$. Each iteration, a new vertex is created and connected to the closest point in $S$ along the shortest possible path. 

If there are obstacles, the tree will go up to the obstacle's boundary, approaching as close as the collision detection algorithm allows.

\begin{algorithm}[t!]
	\caption{Basic RRT}\label{algor:rrt_alg}
	\begin{algorithmic}[1]
		\State $\bm{G}.init(q_{1})$;
		\For{$i=1$ to $K$} 
		\vspace{-2pt}
		\State $\bm{G}.add\_vertex(\alpha(i))$;
		\vspace{-2pt}
		\State $q_n \longleftarrow$ NEAREST$(S(\bm{G},\alpha(i))$;
		\vspace{-2pt}
		\State $\bm{G}.add\_edge(q_n,\alpha(i))$;
		\vspace{-2pt}
		\EndFor 
	\end{algorithmic}
\end{algorithm}

The planner uses the obtained trees to find a path in $\bm{C_{free}}$ between the initial and the desired configuration. There are two different approaches for finding this path (figure \ref{fig:rrt_ex}):
\begin{itemize}
	\item \textbf{Single-tree search:} The planner grows a tree from $\bm{q_1}$ as shown in figure \ref{fig:rrt} and checks in every step if it is possible to reach $\bm{q_G}$ with the $\bm{S}$ created by the tree.
	\item \textbf{Balanced, bidirectional search:} Instead of creating only one RRT, it creates an aditional tree that starts from $\bm{q_G}$, this is quite effective when obstacles are partially surrounding $\bm{q_G}$ or $\bm{q_1}$. The graph $\bm{G}$ is formed by two trees, $T_a$ and $T_b$, growing from $\bm{q_1}$ and $\bm{q_G}$ respectively. Here, $\bm{S}$ is formed by $T_a$ and $T_b$. After some iterations, the trees are swapped, so $T_b$ is now growing around $\bm{q_1}$. Then, $T_b$ connects to a new vertex created for $T_a$, this causes that $T_b$ tries to grow towards $T_a$ and vice-versa. The solution is found when both trees connect.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{rrt_example.png}
	\vspace{-10pt}
	\caption[Single and bidirectional RTT]{ a) Single tree RRT, b) Bidirectional RRT \citep[chap.5, page 219]{planning}.}
	\vspace{-15pt}
	\label{fig:rrt_ex}
\end{figure}

The bidirectional search gives much better performance compared to the single-tree search.

\subsection{Optimization-based motion planning}
\label{sub:optimization}
A different approach to solve the trajectory generation software, is using an optimization algorithm that tries to minimize an error, in this case, the distance between the robot's end effector (EEF) an the goal.
 
The robot motion controller developed in this work uses a model predictive control (MPC) algorithm. Ax explained by \citet{mpc} \textit{"all MPC	systems rely on the idea of generating values for process inputs as solutions of an on-line (real-time) optimization problem"}. 

MCP, also known as receding horizon control, is based on iterative optimization of a plant model. The approach used in this work, is solving the MPC with a sequence of convex quadratic programs (QP), where all motion constraints, goal and robot position are given as input to the problem and joint velocities are obtained as result (section \ref{sec:motion_controller}).

Quagratic programming is the process of solving a linearly constrained quadratic optimization problem of the form:
\begin{equation}
f(x):= \frac{1}{2}x^{T}\textbf{P}x - x^{T}b 
\label{eq:qp}
\end{equation}
The QP minimizes equation \ref{eq:qp} over $x \in \mathbb{R} ^{n}$ subject to the constraints:
$$\textbf{C}x = c $$
$$\textbf{D}x \leq d$$
Where the matrix $\textbf{P} \in \mathbb{R} ^{n \times n}$ is symmetric positive definite. Matrices $\textbf{C}$ and $\textbf{D}$ are of size $\textbf{C} \in \mathbb{R} ^{m \times n}$ and $\textbf{D} \in \mathbb{R} ^{m \times n}$ and vector $b \in \mathbb{R} ^{n}$. Vector  $c \in \mathbb{R} ^{m}$ sets the equality constraints and $d \in \mathbb{R} ^{p}$ the inequality constraints of the system \citep{qp_theory}. There are two approaches for solving this optimization problem, by active set strategies or by interior point methods.

Lets consider a discrete-time, time-invariant, linear system of the form:
\begin{equation}
x_{k+1} =  \textbf{A}x_{k} + \textbf{B}u_{k}
\end{equation} 
where $x_{k} \in \mathbb{R} ^{n_{x}}$ is the system state, $u_{k} \in \mathbb{R} ^{n_{u}}$, the inputs and $\textbf{A} \in \mathbb{R} ^{n_{x} \times n_{x}}$  and $\textbf{B} \in \mathbb{R} ^{n_{x} \times n_{u}}$ are fixed matrices. The constraints of input and state have to be guaranteed:
\begin{equation}
\underline{c} \leq \textbf{C}x_k \leq \overline{c}
\label{eq_c1}
\end{equation} 
\begin{equation}
\underline{d} \leq \textbf{D}u_k \leq \overline{d}
\label{eq_c2}
\end{equation} 
According to \citet{qp_algorithm}, adapting equation \ref{eq:qp} to this system, the optimization problem the MPC solves is:
\begin{equation}
min\ \ x_{N}^{T}\ \textbf{P}x_N + \sum_{i=0}^{N-1} (x_{i}^{T}\ \textbf{Q}\ x_i + u_{i}^{T}\ \textbf{R}\ u_i)
\end{equation} 
subject to the constrains given by equations \ref{eq_c1} and \ref{eq_c2}. The initial state $x_0$ is given as input to the first iteration of the QP. As result, the required input vector $u_0$ is obtained and applied to the system. The state achieved after applying $u_0$ is given as input to the next optimization problem. The MPC successively solves parametric QP to obtain the required input sequence that lead to a desired system state.

\citet{qp_algorithm} developed a strategy for the fast solution of QPs called \textit{online active set strategy}. They assume that the change in states between one QP and the next one is not big and therefore, the solution of the new QP is close to the solution of the old QP. This approach allows faster computations, that can be used in online applications ofcomplex systems.
 
The software \textit{qpOASES} implements this \textit{online active set strategy} for use in MPC, therefore it was used in this work to calculate the joint velocities as explained in section \ref{sec:motion_controller}.

One of the advantages of motion control based on QP \textit{online active set strategy} is the computational speed. The trajectory can be generated fast enough for the system to recalculate it several times during the execution. Therefore, the system can react to unforeseen events, like a change in position of the goal. This is called \textit{On-line Trajectory Generation}.

Using QP for robot motion control has already been done. \cite{acc_level_control} uses QP in a cyclic-motion generation scheme to prevent the angle drift of redundant robot manipulators. They apply the cyclic-motion criterion together with joint angle, velocity and acceleration limits to generate the QP, which is then solved using a neural network.

\todo{small box with QP algorithm}

\subsection{Other On-line Trajectory Generation approaches}

The \textit{Reflexxes Motion Libraries} \citep{reflexxes_lib} are robot motion control libraries that provide tools for trajectory generation, the key capabilities of these libraries are the ability of calculating motions from an arbitrary initial state and the calculation of new motion on every control cycle.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{reflexxes.png}
	\vspace{-10pt}
	\caption[Reflexxes]{Interface of the Reflexxes Motion Libraries \citep[page 1]{reflexxes_lib}.}
	\vspace{-15pt}
	\label{fig:reflex}
\end{figure}

Figure \ref{fig:reflex} shows the interface of the motion library, it considers the current state of the robot and it's constrains to calculate the new state the system needs to achieve the target position and velocity. Doing the calculations on every control cycle (approx 1 msec) allows the system to react to unforseen events, such as switching of coordinate frames, and to sensor signals.

Reflexxes is based on \citet{reflexxes_theory}, they propose a motion control technique that allows instantaneous switching between \textit{trajectory-following control} and  \textit{sensor-guided control}.

\todo{explain the algorithm used here}

\section{Software}
In the last years, the number of applications where humans and robots work in proximity to each other has increased. Robot control has become a wide-spread research area. This has lead to the development of many software programs used for robot manipulation. 

The  institute for Artificial Intelligence (IAI) uses ROS to communicate with the robots. Since the system developed during this thesis is designed for the Boxy robot of the IAI, it was done using ROS as base. For trajectory planning, I developed a controller based on Giskard (section \ref{subsec:giskard}), with RVIZ as a side tool for visualization. The collision detection of the system used Moveit! (section \ref{subsec:moveit}). I also developed a small perception system for detecting the objects the robot will grasp. This was done using the Chilitags fiducial markers (section \ref{subsec:chili})].

\subsection{ROS}
\label{sec:ros}

\textit{Robot Operating System (ROS)} is an open source software that helps developers to create robot applications. According to the official documentation\footnote{\url{http://wiki.ros.org/ROS/Introduction}}, ROS is a meta-operating system, a system that handles other operating systems, it handles hardware abstraction, low-level device control, implementation of commonly-used functionality, message-passing between processes, and package management. ROS also provides a wide range of libraries and tools for robots, such as drivers and visualizers. Some of these libraries are focused on mobility, manipulation and perception tasks.

One of the ROS main components is its communication infrastructure. This has a message passing interface that provides inter-process communication and is seen as a middleware\footnote{\url{http://www.ros.org/core-components/}}. 

The communication consists of a publish/subscribe message passing system, where one program publishes information under a certain topic. All other programs subscribed to this topic will receive the information. A program can be sending information through one or more topics and, at the same time, receiving information from other topics. ROS works as the middleware that manages and distributes these messages.

ROS uses \textit{nodes} to do computations. A node is a process that communicates with other nodes using topics and the parameter server. The main reason behind nodes is the simplification of program codes. A robot is controlled using many nodes, each one in charge of a small task, such as localization or controlling a wheel; this breaks down the complexity of the system into many smaller subsystems (modules) that work independent of each other, communicating through topics and parameters.

Since most applications require several nodes to control all the subsystems, it is useful to have a code that can launch multiple nodes locally and remotely. A \textit{launch file} is a XML configuration file with a *.launch extension, it can specify a set of parameters and nodes to launch. These files can also include other launch files. 


\subsection{RViz}
\label{subsec:rviz}

RViz is a 3D visualization tool for ROS. RViz displays the sensor data and the robot's state information taken from ROS. Using RViz we can visualize the current state of the robot (figure \ref{fig:rviz}), visualize simulated trajectories, and display information from the sensors, such as 3D point-clouds from the Kinect or the image obtained by a camera.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\linewidth]{boxy/rviz_rep.png}
	\vspace{-10pt}
	\caption{RViz visualization of Boxy.}
	\vspace{-15pt}
	\label{fig:rviz}
\end{figure}

RViz uses a \textit{fixed frame} as a static reference for the visualization, the default frame is  \texttt{$\backslash$world}. All the information that RViz receives is displayed with respect to this frame, including: robot's current state, location of objects in the environment, and data from sensors.

Figure \ref{fig:rviz} shows a simulation of the Boxy robot detecting several objects located on a table in front of it, the transformation frames (TF) of both end effectors (EEF) can also be seen.

\subsection{Giskard}
\label{subsec:giskard}
Giskard\footnote{https://github.com/SemRoCo/giskard\_core} is a constraint- and optimization-based framework (section \ref{sub:optimization}) for robot motion control developed by the Institute for Artificial Intelligence\footnote{http://ai.uni-bremen.de/}. It uses Quadratic Programming (QP)  for solving the motion control problem ()section \ref{sub:optimization}). This controller can react to unexpected events, since it calculates the next trajectory step on every control cycle.  Giskard was used as base to develop the motion controller used in this work (section \ref{sec:motion_controller}), which also uses QP to generate a trajectory.

This controller allows using action models that inform the robot how to execute a specific motion, such as pouring or grasping. The \textit{giscard\_boxy}\footnote{https://github.com/SemRoCo/giskard\_boxy} package contains files with specific configuration for using the \textit{giskard} motion controllers of the Boxy robot, which is the robot used in this thesis.

\subsection{Chilitags}
\label{subsec:chili}

\textit{Chilitags}\footnote{http://chili.epfl.ch/software} is a software library for the detection and identification of 2D fiducial markers for robotics and augmented reality. Fiducial markers are objects placed in the view field of a perception system to be used as a reference. In other words, \textit{Chilitags} are markers added to objects used as reference points in a perception system. In this case, the markers help not only to find the relative position and orientation of the objects with respect to the camera, but also to identify them.
\begin{figure}[H]
	\centering \vspace{-10pt}
	\begin{subfigure}[][Tag 0]
		{\includegraphics[width=0.2\linewidth]{0.png}}
	\end{subfigure}
	\begin{subfigure}[][Tag 1]
		{\includegraphics[width=0.2\linewidth]{1.png}}
	\end{subfigure}
	\begin{subfigure}[][Tag 2]
		{\includegraphics[width=0.2\linewidth]{2.png}}
	\end{subfigure}
	\vspace{-10pt}
	\caption[Example of Chilitags fiducial markers]{Example of Chilitags fiducial markers, each tag has a different ID number.}
	\vspace{-10pt}
	\label{fig:chilitag}
\end{figure}

Figure \ref{fig:chilitag} shows three fiducial markers that were placed in different objects. The precise size of the markers is known beforehand, so when placed in front of a camera, the perception system calculates the exact distance to the marker and identifies it using the pattern.

In this project, perception was done by attaching markers to several kitchen objects the robot had to grasp and detecting the using a Microsoft Kinect 2 placed on top of the robot (see figure \ref{fig:boxy}).

\subsection{Naive Kinematic Simulator}
\label{sub:naive}
The \textit{iai\_naive\_kinematic\_simulator}\footnote{https://github.com/code-iai/iai\_naive\_kinematics\_sim} is a kinematic simulator for robots based on ROS. It was developed by the Institute for Artificial intelligence.

It is a light-weight that does not consider inertia or friction, therefore kinematics algorithms can use it for fast calculations. In this thesis, it is used to simulate the commands generated by the motion controller and generate a trajectory (section \ref{sec:motion_controller}).

\subsection{MoveIt!}
\label{subsec:moveit}

In \citet{moveit}, MoveIt! is defined as "\textit{a set of software	packages integrated with the Robot Operating System (ROS) and designed specifically to provide such capabilities (avoid collisions with humans and other obstacles), especially for mobile manipulation.}".

MoveIt! Planning Scene\footnote{http://docs.ros.org/indigo/api/moveit\_core/html/classplanning\_\_scene\_1\_1PlanningScene.html} is able to create an environment where the robot and external objects can be loaded. These objects can be seen as obstacles or as objects the robot will interact with. Moveit! represents the robot using a kinematic description of it, an URDF (section \ref{sec:urdf}). The environment is described using an Octomap; an Octomap is a probabilistic 3D mapping framework based on octrees, it provides a 3D occupancy grid that defines which parts of the environment are free.

It is important to mention that MoveIt! uses 2 models for each object, as shown in figure \ref{fig:models}, one for visualization and one for collision. 
\begin{figure}[H]
	\centering
	\begin{subfigure}[][Visualization Model]
		{\includegraphics[width=0.4\linewidth]{boxy/visual.png}}
	\end{subfigure}
	\begin{subfigure}[][Collision Model]
		{\includegraphics[width=0.4\linewidth]{boxy/collision.png}}
	\end{subfigure}
	\vspace{-15pt}
	\caption{Models of Boxy used by MoveIt!}
	\vspace{-10pt}
	\label{fig:models}
\end{figure}

The \textit{visualization model} is used by RViz (section \ref{subsec:rviz}) to visualize the robot in the computer. This model is normally as close as possible to the real robot, that means the geometry tends to be complex and the files containing it are heavy. We use this model for visualizing the planned trajectory before executing it, or for visualizing the current state of the robot.

The \textit{collision model} of Boxy contains simplified geometry of the robot, it is usually slightly bigger than the real robot for security reasons. The collision model is the one MoveIt! uses to check for collisions. Since the collision check is done for every pose in the trajectory and for every part in the model, it is convenient to have simplified geometry to speed up the calculations. 

\section{Robot Description}
\label{sec:urdf}

In oder to calculate the position and movements of the robot, all it's elements must be  represented in a way that the robot controller and the trajectory planner understands them. This implies models or descriptions where the geometry, movement ranges and kinematics are described. Such description is made with a Unified Robot Description Format (URDF).

As the name implies, a URDF is a file used to describe a robot, it contains XML specifications of the robot's geometry, kinematics, inertial, and sensing properties. It is the native robot description format in ROS. The URDF loads meshes that contains the geometry of one or several robot links and describes it's relative position with respect to the previous link, it also specifies the type and range of movement it has.

Instead of writing a URDF file, programmers normally write a XACRO of the file, a XACRO is a XML macro, which is easier to read and allows code reuse. 